{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unet with Tversky loss\n",
    "\n",
    "## 10/03/21\n",
    "\n",
    "Asymetric loss functions are an approach to force a semantic segmenter to focus on a rare set of pixels (in this case eggs). One such function is [Tversky loss](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8573779) that can be forumlated as:\n",
    "\n",
    "$$\n",
    "    T(\\alpha, \\beta) = \\frac{tp}{tp + \\alpha*fp + \\beta*fn}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import cv2\n",
    "import segmentation_models_pytorch as smp\n",
    "from utils.datatools import PlanktonDataset\n",
    "from utils.transform_helpers import *\n",
    "from utils.visualizers import visualize\n",
    "from utils.custom_loss import TverskyLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries\n",
    "Point to the dataset split by profile for the ResNet training a few weeks ago, desired network, encoder depth, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file paths\n",
    "vocpath = '/home/eorenstein/VOCCopepodEgg'\n",
    "imgset = 'SplitByProfile-230221'\n",
    "\n",
    "# Unet settings\n",
    "ENCODER = 'resnet18'\n",
    "WEIGHTS = 'imagenet'\n",
    "DEPTH = 5  # desired depth of encoder. default is 5 for resnet-18\n",
    "DECODE_CHAN = (256, 128, 64, 32, 16)  # must set corresponding filter sizes of decoder. length must match depth\n",
    "CHANNELS = 3  # set as 1 for gray, 3 for color\n",
    "ACTIVE = 'softmax2d'\n",
    "CLASSES = ['copepod', 'eggs']\n",
    "n_classes = 1 if len(CLASSES) == 1 else ((len(CLASSES) + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instatiate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEPTH == 5:\n",
    "    model = smp.Unet(\n",
    "        encoder_name=ENCODER,\n",
    "        encoder_weights=WEIGHTS,\n",
    "        in_channels=CHANNELS,  \n",
    "        classes=n_classes,\n",
    "        activation=ACTIVE\n",
    "    )\n",
    "else:\n",
    "    model = smp.Unet(\n",
    "        encoder_name=ENCODER,\n",
    "        encoder_weights=WEIGHTS,\n",
    "        encoder_depth=DEPTH,\n",
    "        decoder_channels=DECODE_CHAN,\n",
    "        in_channels=CHANNELS,  \n",
    "        classes=n_classes,\n",
    "        activation=ACTIVE\n",
    "    )\n",
    "\n",
    "# this is the preprocessing for imagenet if using 3 channel images\n",
    "if CHANNELS == 3:\n",
    "    preprocess = smp.encoders.get_preprocessing_fn(ENCODER, WEIGHTS)\n",
    "    dmy_clf = True\n",
    "else:\n",
    "    preprocess = None\n",
    "    dmy_clf = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get datasets and loaders\n",
    "\n",
    "Since only interested in egg data point to that train and val set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set: 1330 images\n",
      "val set: 313 images\n"
     ]
    }
   ],
   "source": [
    "training_dataset = PlanktonDataset(root=vocpath,\n",
    "                                   img_set=f'{imgset}/egg_train.txt',\n",
    "                                   augs=training_transform(),\n",
    "                                   preproc=get_preprocessing(preprocess),\n",
    "                                   classes=CLASSES,\n",
    "                                   dummy_clf=dmy_clf\n",
    "                                  )\n",
    "\n",
    "val_dataset = PlanktonDataset(root=vocpath,\n",
    "                              img_set=f'{imgset}/egg_val.txt',\n",
    "                              augs=validation_transform(),\n",
    "                              preproc=get_preprocessing(preprocess),\n",
    "                              classes=CLASSES,\n",
    "                              dummy_clf=dmy_clf\n",
    "                             )\n",
    "\n",
    "# check that the lengths are consistent\n",
    "print('train set:', len(training_dataset), 'images')\n",
    "print('val set:', len(val_dataset), 'images')\n",
    "\n",
    "train_loader = DataLoader(training_dataset, batch_size=8, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss function\n",
    "\n",
    "This is where the loss function is defined. Try out the custom Tversky loss just on the egg channel (ignore the without-egg and background channel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = TverskyLoss(ignore_channels=[0,2])\n",
    "\n",
    "metrics = [\n",
    "    smp.utils.metrics.IoU(threshold=0.5, ignore_channels=[0,2]),\n",
    "    smp.utils.metrics.Fscore(threshold=0.5, ignore_channels=[1, 2]),\n",
    "    smp.utils.metrics.Accuracy(threshold=0.5, ignore_channels=[1, 2])\n",
    "]\n",
    "\n",
    "# standard atom optimizer from PyTorch (might want to consider reducing the learning rate)\n",
    "optimizer = torch.optim.Adam([ \n",
    "    dict(params=model.parameters(), lr=0.0001),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the epoch runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epoch = smp.utils.train.TrainEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    optimizer=optimizer,\n",
    "    device='cuda',\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "valid_epoch = smp.utils.train.ValidEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    device='cuda',\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "train: 100%|██████████| 167/167 [00:06<00:00, 23.86it/s, tversky_loss - 0.6541, iou_score - 0.2974, fscore - 0.01599, accuracy - 0.5342]\n",
      "valid: 100%|██████████| 313/313 [00:02<00:00, 113.34it/s, tversky_loss - 0.461, iou_score - 0.3892, fscore - 0.01859, accuracy - 0.5262] \n",
      "Model saved!\n",
      "\n",
      "Epoch: 1\n",
      "train: 100%|██████████| 167/167 [00:06<00:00, 25.67it/s, tversky_loss - 0.3943, iou_score - 0.3996, fscore - 0.03059, accuracy - 0.4938]\n",
      "valid: 100%|██████████| 313/313 [00:02<00:00, 112.23it/s, tversky_loss - 0.3595, iou_score - 0.4452, fscore - 0.04419, accuracy - 0.526] \n",
      "Model saved!\n",
      "\n",
      "Epoch: 2\n",
      "train: 100%|██████████| 167/167 [00:06<00:00, 26.19it/s, tversky_loss - 0.2975, iou_score - 0.5089, fscore - 0.06357, accuracy - 0.5088]\n",
      "valid: 100%|██████████| 313/313 [00:02<00:00, 121.26it/s, tversky_loss - 0.3017, iou_score - 0.611, fscore - 0.08842, accuracy - 0.5751] \n",
      "Model saved!\n",
      "\n",
      "Epoch: 3\n",
      "train: 100%|██████████| 167/167 [00:06<00:00, 25.99it/s, tversky_loss - 0.2461, iou_score - 0.5646, fscore - 0.07142, accuracy - 0.5142]\n",
      "valid: 100%|██████████| 313/313 [00:02<00:00, 110.86it/s, tversky_loss - 0.2321, iou_score - 0.633, fscore - 0.08654, accuracy - 0.583]  \n",
      "Model saved!\n",
      "\n",
      "Epoch: 4\n",
      "train: 100%|██████████| 167/167 [00:06<00:00, 25.44it/s, tversky_loss - 0.2321, iou_score - 0.5846, fscore - 0.07672, accuracy - 0.5162]\n",
      "valid: 100%|██████████| 313/313 [00:02<00:00, 111.76it/s, tversky_loss - 0.2184, iou_score - 0.654, fscore - 0.08247, accuracy - 0.606]  \n",
      "Model saved!\n",
      "\n",
      "Epoch: 5\n",
      "train: 100%|██████████| 167/167 [00:06<00:00, 25.70it/s, tversky_loss - 0.2187, iou_score - 0.5983, fscore - 0.07623, accuracy - 0.5237]\n",
      "valid: 100%|██████████| 313/313 [00:02<00:00, 115.63it/s, tversky_loss - 0.2053, iou_score - 0.6595, fscore - 0.09154, accuracy - 0.5781]\n",
      "Model saved!\n",
      "\n",
      "Epoch: 6\n",
      "train: 100%|██████████| 167/167 [00:06<00:00, 26.09it/s, tversky_loss - 0.2085, iou_score - 0.6122, fscore - 0.07637, accuracy - 0.5223]\n",
      "valid: 100%|██████████| 313/313 [00:02<00:00, 119.75it/s, tversky_loss - 0.1968, iou_score - 0.6597, fscore - 0.07973, accuracy - 0.588] \n",
      "Model saved!\n",
      "\n",
      "Epoch: 7\n",
      "train: 100%|██████████| 167/167 [00:06<00:00, 25.49it/s, tversky_loss - 0.2034, iou_score - 0.6197, fscore - 0.08326, accuracy - 0.5249]\n",
      "valid: 100%|██████████| 313/313 [00:02<00:00, 114.25it/s, tversky_loss - 0.1814, iou_score - 0.6791, fscore - 0.08591, accuracy - 0.5921]\n",
      "Model saved!\n",
      "\n",
      "Epoch: 8\n",
      "train: 100%|██████████| 167/167 [00:06<00:00, 25.62it/s, tversky_loss - 0.1994, iou_score - 0.6205, fscore - 0.07984, accuracy - 0.5293]\n",
      "valid: 100%|██████████| 313/313 [00:02<00:00, 113.68it/s, tversky_loss - 0.1731, iou_score - 0.6896, fscore - 0.08375, accuracy - 0.5666]\n",
      "Model saved!\n",
      "\n",
      "Epoch: 9\n",
      "train: 100%|██████████| 167/167 [00:06<00:00, 25.53it/s, tversky_loss - 0.1925, iou_score - 0.633, fscore - 0.07486, accuracy - 0.5186] \n",
      "valid: 100%|██████████| 313/313 [00:02<00:00, 112.34it/s, tversky_loss - 0.1797, iou_score - 0.7112, fscore - 0.09111, accuracy - 0.6016]\n",
      "Model saved!\n",
      "\n",
      "Epoch: 10\n",
      "train: 100%|██████████| 167/167 [00:05<00:00, 28.66it/s, tversky_loss - 0.187, iou_score - 0.6416, fscore - 0.08062, accuracy - 0.5294] \n",
      "valid: 100%|██████████| 313/313 [00:02<00:00, 112.06it/s, tversky_loss - 0.2046, iou_score - 0.658, fscore - 0.084, accuracy - 0.5674]   \n",
      "\n",
      "Epoch: 11\n",
      "train: 100%|██████████| 167/167 [00:06<00:00, 25.78it/s, tversky_loss - 0.1934, iou_score - 0.6298, fscore - 0.07889, accuracy - 0.5225]\n",
      "valid: 100%|██████████| 313/313 [00:02<00:00, 110.26it/s, tversky_loss - 0.1858, iou_score - 0.6958, fscore - 0.0929, accuracy - 0.5949] \n",
      "\n",
      "Epoch: 12\n",
      "train: 100%|██████████| 167/167 [00:06<00:00, 25.63it/s, tversky_loss - 0.1772, iou_score - 0.6532, fscore - 0.08039, accuracy - 0.5209]\n",
      "valid: 100%|██████████| 313/313 [00:02<00:00, 113.99it/s, tversky_loss - 0.1701, iou_score - 0.707, fscore - 0.09009, accuracy - 0.5754] \n",
      "\n",
      "Epoch: 13\n",
      "train: 100%|██████████| 167/167 [00:06<00:00, 25.79it/s, tversky_loss - 0.1738, iou_score - 0.6621, fscore - 0.08225, accuracy - 0.5224]\n",
      "valid: 100%|██████████| 313/313 [00:02<00:00, 111.27it/s, tversky_loss - 0.1687, iou_score - 0.7183, fscore - 0.1013, accuracy - 0.6097]\n",
      "Model saved!\n",
      "\n",
      "Epoch: 14\n",
      "train: 100%|██████████| 167/167 [00:06<00:00, 25.69it/s, tversky_loss - 0.1792, iou_score - 0.6496, fscore - 0.0858, accuracy - 0.5363] \n",
      "valid: 100%|██████████| 313/313 [00:02<00:00, 111.68it/s, tversky_loss - 0.1614, iou_score - 0.7262, fscore - 0.09703, accuracy - 0.5971]\n",
      "Model saved!\n"
     ]
    }
   ],
   "source": [
    "max_score = 0\n",
    "\n",
    "# where to save\n",
    "outpath = '/home/eorenstein/python_code/eggs-emantic/clf-outputs'\n",
    "descript = 'eggs_only_tversky'  # info about model\n",
    "\n",
    "for i in range(0, 15):\n",
    "    \n",
    "    print('\\nEpoch: {}'.format(i))\n",
    "    train_logs = train_epoch.run(train_loader)\n",
    "    valid_logs = valid_epoch.run(val_loader)\n",
    "    \n",
    "    # do something (save model, change lr, etc.)\n",
    "    if max_score < valid_logs['iou_score']:\n",
    "        max_score = valid_logs['iou_score']\n",
    "        torch.save(model, os.path.join(outpath,f'{descript}_best_model.pth'))\n",
    "        print('Model saved!')\n",
    "        \n",
    "    if max_score > 0.94:\n",
    "        optimizer.param_groups[0]['lr'] = 1e-5\n",
    "        print('Decrease decoder learning rate to 1e-5!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on the independent test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for running images through the classifier\n",
    "test_dataset = PlanktonDataset(root=vocpath, \n",
    "                               img_set=f'{imgset}/egg_test.txt',\n",
    "                               augs=validation_transform(),\n",
    "                               preproc=get_preprocessing(preprocess),\n",
    "                               classes=CLASSES,\n",
    "                               dummy_clf=dmy_clf)\n",
    "\n",
    "# this does not do any preprocessing for plotting\n",
    "test_viz = PlanktonDataset(root=vocpath, \n",
    "                           img_set=f'{imgset}/egg_test.txt',\n",
    "                           classes=CLASSES,\n",
    "                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model on all the images of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid: 100%|██████████| 266/266 [00:02<00:00, 123.23it/s, tversky_loss - 0.2067, iou_score - 0.5728, fscore - 0.7261, accuracy - 0.5937]\n"
     ]
    }
   ],
   "source": [
    "best_model = torch.load(f'./clf-outputs/{descript}_best_model.pth')\n",
    "\n",
    "metrics = [\n",
    "    smp.utils.metrics.IoU(threshold=0.5, ignore_channels=[0, 1]),\n",
    "    smp.utils.metrics.Fscore(threshold=0.5, ignore_channels=[0, 1]),\n",
    "    smp.utils.metrics.Accuracy(threshold=0.5, ignore_channels=[0, 1])\n",
    "]\n",
    "\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset)\n",
    "\n",
    "test_epoch = smp.utils.train.ValidEpoch(\n",
    "    best_model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    device='cuda',\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "logs = test_epoch.run(test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on images without eggs\n",
    "\n",
    "Load in images without eggs to see what it does. First grab a list of image-ids of just copepods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13458\n"
     ]
    }
   ],
   "source": [
    "with open('/home/eorenstein/VOCCopepodEgg/ImageSets/Main/SplitByProfile-230221/cope_test.txt', 'r') as ff:\n",
    "    copes = list(ff)\n",
    "    ff.close()\n",
    "    \n",
    "copes = [line.strip() for line in copes]\n",
    "print(len(copes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more in there than we care to process. Select a random 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(copes)\n",
    "copes = copes[0:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load them as a dataset and procede as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for running images through the classifier\n",
    "cope_dataset = PlanktonDataset(root=vocpath, \n",
    "                               img_set=copes,\n",
    "                               augs=validation_transform(),\n",
    "                               preproc=get_preprocessing(preprocess),\n",
    "                               classes=CLASSES,\n",
    "                               dummy_clf=dmy_clf)\n",
    "\n",
    "# this does not do any preprocessing for plotting\n",
    "cope_viz = PlanktonDataset(root=vocpath, \n",
    "                           img_set=copes,\n",
    "                           classes=CLASSES,\n",
    "                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-instantiate in case not loaded and evaluate the model on all the images of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid: 100%|██████████| 1000/1000 [00:08<00:00, 113.61it/s, tversky_loss - 1.0, iou_score - 0.5655, fscore - 0.7175, accuracy - 0.5849]\n"
     ]
    }
   ],
   "source": [
    "metrics = [\n",
    "    smp.utils.metrics.IoU(threshold=0.5, ignore_channels=[0, 1]),\n",
    "    smp.utils.metrics.Fscore(threshold=0.5, ignore_channels=[0, 1]),\n",
    "    smp.utils.metrics.Accuracy(threshold=0.5, ignore_channels=[0, 1])\n",
    "]\n",
    "\n",
    "cope_dataloader = DataLoader(cope_dataset)\n",
    "\n",
    "cope_epoch = smp.utils.train.ValidEpoch(\n",
    "    best_model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    device='cuda',\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "logs = cope_epoch.run(cope_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n"
     ]
    }
   ],
   "source": [
    "egg_px = []\n",
    "cope_px = []\n",
    "num = 0\n",
    "\n",
    "for jj in range(len(cope_dataset)):\n",
    "    img, mask = cope_dataset[jj]\n",
    "\n",
    "    mask = mask.squeeze()\n",
    "\n",
    "    # put it on the GPU\n",
    "    imgtens = torch.from_numpy(img).to('cuda').unsqueeze(0)\n",
    "    pred = best_model.predict(imgtens)\n",
    "    pred = (pred.squeeze().cpu().numpy().round())\n",
    "    \n",
    "    egg_px.append(np.sum(pred[1, :, :]))\n",
    "    cope_px.append(np.sum(pred[0, :, :]))\n",
    "    \n",
    "    if np.sum(pred[1, :, :]) < 50:\n",
    "        num+=1\n",
    "\"\"\"\n",
    "fig, ax = plt.subplots(1,2, figsize=(16, 5))\n",
    "ax[0].hist([egg_px, cope_px])\n",
    "ax[1].hist(cope_px)\n",
    "\"\"\"\n",
    "print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([161., 314., 283., 128.,  72.,  23.,  10.,   5.,   2.,   2.]),\n",
       " array([   0. ,  141.3,  282.6,  423.9,  565.2,  706.5,  847.8,  989.1,\n",
       "        1130.4, 1271.7, 1413. ], dtype=float32),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(egg_px)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eggs-emantic",
   "language": "python",
   "name": "eggs-emantic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
